{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Autoencoder with Annoy for CBIR\n",
    "\n",
    "## Grayscale 32x64 pixels, 4x8x128 encoder convolutions, GlobalAverage to 128 dimensions\n",
    "\n",
    "#### Results: Grayscale, even at this resolution seems MUCH better than Black and White (pure through resize or not) or Edge Detection. Unexpected.  But MUCH better at grouping pills through rotations to one another. \n",
    "\n",
    "\n",
    "From Annoy:\\ - see below:\n",
    "Number of incorrect pills:       0 out of 24 pills\n",
    "Number of incorrect pill images: 0 out of 96 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15081321793875527189\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#target_image_size = (64,112)\n",
    "target_image_size = (32,64)\n",
    "\n",
    "color_mode='grayscale' # 'grayscale' or 'rgb'\n",
    "\n",
    "if color_mode=='grayscale':\n",
    "    target_image_size_3D = (target_image_size[0], target_image_size[1], 1)\n",
    "else: #then rgb\n",
    "    target_image_size_3D = (target_image_size[0], target_image_size[1], 3)\n",
    "\n",
    "batch_size_training = 32\n",
    "batch_size_validation = 32\n",
    "\n",
    "# Directories\n",
    "image_dir_base = 'data_with_rotations'\n",
    "image_dir_training = image_dir_base + '/train'\n",
    "image_dir_validation = image_dir_base + '/validate'\n",
    "image_dir_testing = image_dir_base + '/test'\n",
    "image_dir_samples = image_dir_testing\n",
    "\n",
    "# Training variables\n",
    "training_steps_per_epoch = 200\n",
    "training_number_of_epoch = 100\n",
    "validation_steps = 200\n",
    "training_early_stop_patience = 5\n",
    "\n",
    "# Model name to save\n",
    "model_name='autoencoder_v1.0_36x64-4x8x128'\n",
    "\n",
    "# Model name to load for Testing\n",
    "model_name_pretrained = model_name\n",
    "\n",
    "# Encoder model name\n",
    "model_name_encoder = model_name + '-encoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required items for training\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Dense, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras import callbacks\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, matplotlib, cv2, etc only print in greyscale \n",
    "#   when you have 3 color axes (RGB) all set to make the image look grey.\n",
    "#   But Keras loads greyscale images with only a single number (to optimize training, etc)\n",
    "#   So, we need to convert any Keras greyscale images to have 3 values\n",
    "\n",
    "def display_image(single_image):\n",
    "    #check to see if image is rgb\n",
    "    if (np.shape(single_image)[-1]==3):\n",
    "        plt.imshow(single_image)\n",
    "    if (np.shape(single_image)[-1]==1):\n",
    "        si = np.concatenate((single_image,single_image,single_image), axis=2)\n",
    "        plt.imshow(si)\n",
    "        \n",
    "def get_image(single_image):\n",
    "    #check to see if image is rgb\n",
    "    if (np.shape(single_image)[-1]==3):\n",
    "        return single_image\n",
    "    if (np.shape(single_image)[-1]==1):\n",
    "        si = np.concatenate((single_image,single_image,single_image), axis=2)\n",
    "        return si\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageDataGenerators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 722 images belonging to 1 classes.\n",
      "Found 191 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Created the Train and Validation image generators\n",
    "\n",
    "# Load the data (in that case MNIST)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        #shear_range=0.05,\n",
    "        #zoom_range=0.01,\n",
    "        #rotation_range=5.00,\n",
    "        #height_shift_range=0.10,\n",
    "        #width_shift_range=0.10,\n",
    "        rescale=1. / 255,\n",
    "        horizontal_flip=False)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        image_dir_training,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=batch_size_training,\n",
    "        class_mode='input',\n",
    "        color_mode=color_mode)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        image_dir_validation,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=batch_size_validation,\n",
    "        class_mode='input',\n",
    "        color_mode=color_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an Sample image generator and get sample images to use throughout all the training for visualization\n",
    "\n",
    "\n",
    "# Create callback function to use later.\n",
    "sample_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "sample_generator = sample_datagen.flow_from_directory(\n",
    "        image_dir_samples,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=16,\n",
    "        class_mode='input',\n",
    "        color_mode=color_mode)\n",
    "\n",
    "next_batch = next(sample_generator)\n",
    "sample_images = next_batch[0]\n",
    "test_image=sample_images[1]\n",
    "ti = test_image\n",
    "\n",
    "#\n",
    "display_image(test_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model for Training - Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    \n",
    "    \n",
    "    #Input\n",
    "    input_img = Input(shape=target_image_size_3D, name='input')  # adapt this if using `channels_first` image data format\n",
    "    \n",
    "     # Layer 10\n",
    "    x = Conv2D(16, (5, 5), activation='relu', padding='same', strides=(2,2))(input_img)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Layer 20\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', strides=(2,2))(x)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "#    # Added\n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "#    \n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # Layer 30\n",
    "\n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Layer 40\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same', name='encoder')(x)\n",
    "\n",
    "    # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "    # Uplayer 40\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    # Uplayer 30\n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "\n",
    "#    #Added\n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = UpSampling2D((2, 2))(x)  \n",
    "#    \n",
    "#    x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "#    #x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = UpSampling2D((2, 2))(x)  \n",
    "    \n",
    "    # Uplayer 20\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    # Uplayer 10\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    # Output\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='decoded')(x)\n",
    "\n",
    "\n",
    "    autoencoder = Model(input_img, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "    print(autoencoder.outputs)\n",
    "        \n",
    "    return autoencoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback to write sample images to disk\n",
    "class ProgressCallback(callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, sample_image):\n",
    "        self.sample_image = sample_image\n",
    "        self.image4d = self.sample_image[None,:] # predict needs a batch of images (shape=4). This adds a dimension  \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        processed_images = self.model.predict(x=[self.image4d],batch_size=1)\n",
    "         \n",
    "        # plot the image and save it\n",
    "        f = plt.figure()\n",
    "        f.add_subplot(1, 2, 1)  # this line outputs images side-by-side\n",
    "        sim = get_image(self.sample_image)\n",
    "        plt.imshow(sim)\n",
    "        f.add_subplot(1, 2, 2)  # this line outputs images side-by-side\n",
    "        pim = get_image(processed_images[0])\n",
    "        plt.imshow(pim)\n",
    "        plt.suptitle('Epoch ' + str(epoch))\n",
    "        filename = 'epoch-' + str(epoch) + '.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model_to_train):\n",
    "    progress = ProgressCallback(sample_image=sample_images[0])\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=training_early_stop_patience,\n",
    "                              verbose=0, mode='auto')\n",
    "    model_to_train.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=training_steps_per_epoch,\n",
    "        epochs=training_number_of_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False),progress,early_stop])\n",
    "    \n",
    "    model_to_train.save(model_name + '.h5')\n",
    "    \n",
    "    return model_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'decoded_1/Sigmoid:0' shape=(?, 32, 64, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "x = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 32, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 32, 16)        416       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 16, 32)         4640      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 16, 128)        36992     \n",
      "_________________________________________________________________\n",
      "encoder (MaxPooling2D)       (None, 4, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 16, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 16, 32)         36896     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 32, 16)        4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 32, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "decoded (Conv2D)             (None, 32, 64, 1)         145       \n",
      "=================================================================\n",
      "Total params: 231,297\n",
      "Trainable params: 231,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'encoder_1/MaxPool:0' shape=(?, 4, 8, 128) dtype=float32>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show encoder output size (before global pooling)\n",
    "e = x.get_layer('encoder')\n",
    "e.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 40/200 [=====>........................] - ETA: 19s - loss: 0.4904"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-3f97ab21a130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-d0de1e3f3a8d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_to_train)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False),progress,early_stop])\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_to_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder = x\n",
    "autoencoder = train_model(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install jupyter-tensorboard\n",
    "for i in range(3):\n",
    "    pass\n",
    "    #autoencoder = train_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    #autoencoder = train_model(x)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "#import cv2\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model :\n",
      "Model loaded in:  1.6216092109680176\n"
     ]
    }
   ],
   "source": [
    "# Load the model trained above\n",
    "print('Loading model :')\n",
    "t0 = time.time()\n",
    "autoencoder = load_model(model_name_pretrained + '.h5')\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoder').output)\n",
    "t1 = time.time()\n",
    "print('Model loaded in: ', t1-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convenience methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_directory_images_generator:\n",
    "    def __init__(self, sourcedir='data_with_rotations/test', batch_size=16, color_mode='grayscale', target_image_size=(100,100)):\n",
    "        self.batch_size = batch_size\n",
    "        self.sourcedir = sourcedir\n",
    "        self.color_mode = color_mode\n",
    "        self.target_image_size = target_image_size\n",
    "        \n",
    "        self.test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        self.test_generator = self.test_datagen.flow_from_directory(\n",
    "                sourcedir,\n",
    "                target_size=target_image_size,\n",
    "                batch_size=self.batch_size,\n",
    "                class_mode='input',\n",
    "                color_mode=self.color_mode,\n",
    "                shuffle=False)\n",
    "        \n",
    "        self.n = self.test_generator.n\n",
    "        self.filenames = self.test_generator.filenames   \n",
    "        self.current_batch = 0\n",
    "        self.max_batch = int(self.n / self.batch_size)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        bi = self.test_generator.batch_index\n",
    "        bs = self.test_generator.batch_size\n",
    "        batch_file_names = self.test_generator.filenames[bi*bs:bi*bs+bs]\n",
    "        return (next(self.test_generator), batch_file_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model from the autoencoder, only up to the embedding layer\n",
    "enc_model = Model(autoencoder.input, autoencoder.get_layer('encoder').output)\n",
    "\n",
    "x1 = enc_model.get_layer('encoder').output\n",
    "#x1 = GlobalMaxPooling2D(name='flat')(x1)\n",
    "x1 = GlobalAveragePooling2D(name='flat')(x1)\n",
    "encoder = Model(enc_model.input, x1)\n",
    "\n",
    "# save the model to disk for reuse later\n",
    "encoder.save(model_name_encoder + '.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8100 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary with all filenames (keys) and predicted encodings (values)\n",
    "\n",
    "image_dir_testing = 'data_with_many_rotations2/train'\n",
    "\n",
    "image_encoding_dict = {}\n",
    "\n",
    "test_images = all_directory_images_generator(batch_size=45, target_image_size=target_image_size, sourcedir=image_dir_testing)\n",
    "bs = test_images.batch_size\n",
    "\n",
    "for i in range(test_images.max_batch):\n",
    "    images_both_x_and_y, names = next(test_images)\n",
    "    images = images_both_x_and_y[0]\n",
    "    encodings = encoder.predict(images,batch_size=bs)\n",
    "    for j in range(bs):\n",
    "        image_encoding_dict[names[j]]=encodings[j]\n",
    "        \n",
    "\n",
    "\n",
    "# Create dictionary with index integers (keys) and filenames (values)\n",
    "#   This is needed later for Annoy, since it uses integers as item keys, and we have to map back to a filename\n",
    "image_filename_dict = {}\n",
    "\n",
    "i = 0\n",
    "for key, value in image_encoding_dict.items():\n",
    "    image_filename_dict[i]=key\n",
    "    i = i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/100_0.jpg\n"
     ]
    }
   ],
   "source": [
    "# Let's just view a few input images and output images from the autoencoder\n",
    "#   This includes the decoder part of the autoencoder, so we'll still see an image\n",
    "\n",
    "next_batch, next_filenames = next(test_images) # Use the test_images defined above\n",
    "#next_batch = next(test_generator)\n",
    "images = next_batch[0]\n",
    "first_image=images[0]\n",
    "second_image=images[1]\n",
    "\n",
    "plt.imshow(get_image(first_image))\n",
    "plt.show()\n",
    "\n",
    "#plt.imshow(second_image)\n",
    "#plt.show()\n",
    "\n",
    "recreated_pill = autoencoder.predict(x=images,batch_size=32)\n",
    "\n",
    "plt.imshow(get_image(recreated_pill[0]))\n",
    "plt.show()\n",
    "print(next_filenames[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feel for the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataframe from the dictionary\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(image_encoding_dict, orient='index')\n",
    "\n",
    "#df['filename'] = df.index\n",
    "#df['image']= df['filename'].apply(lambda x: x.split('/')[-1])\n",
    "#HOLY COW!  All of these are so close together. :-(  Can't differentiate one from another easily - Deeper network help?\n",
    "#df.to_csv('96_images_32x64_AvgPool_4x8.csv')\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.116975</td>\n",
       "      <td>0.192254</td>\n",
       "      <td>0.270316</td>\n",
       "      <td>0.055375</td>\n",
       "      <td>0.067886</td>\n",
       "      <td>0.159494</td>\n",
       "      <td>0.025392</td>\n",
       "      <td>0.193006</td>\n",
       "      <td>0.220286</td>\n",
       "      <td>0.034857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046615</td>\n",
       "      <td>0.083562</td>\n",
       "      <td>0.035388</td>\n",
       "      <td>0.099931</td>\n",
       "      <td>0.126097</td>\n",
       "      <td>0.253255</td>\n",
       "      <td>0.084274</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.110998</td>\n",
       "      <td>0.175048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.037431</td>\n",
       "      <td>0.030156</td>\n",
       "      <td>0.049625</td>\n",
       "      <td>0.010156</td>\n",
       "      <td>0.015720</td>\n",
       "      <td>0.048220</td>\n",
       "      <td>0.012440</td>\n",
       "      <td>0.026172</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017693</td>\n",
       "      <td>0.031684</td>\n",
       "      <td>0.015684</td>\n",
       "      <td>0.043736</td>\n",
       "      <td>0.049993</td>\n",
       "      <td>0.037211</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.076867</td>\n",
       "      <td>0.055959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.024034</td>\n",
       "      <td>0.074731</td>\n",
       "      <td>0.129599</td>\n",
       "      <td>0.026587</td>\n",
       "      <td>0.022601</td>\n",
       "      <td>0.082664</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.107948</td>\n",
       "      <td>0.156730</td>\n",
       "      <td>0.013670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.007219</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>0.030014</td>\n",
       "      <td>0.005417</td>\n",
       "      <td>0.169529</td>\n",
       "      <td>0.015992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010813</td>\n",
       "      <td>0.005754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.087276</td>\n",
       "      <td>0.181227</td>\n",
       "      <td>0.227657</td>\n",
       "      <td>0.046315</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.124366</td>\n",
       "      <td>0.017765</td>\n",
       "      <td>0.175341</td>\n",
       "      <td>0.205185</td>\n",
       "      <td>0.025137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036601</td>\n",
       "      <td>0.065289</td>\n",
       "      <td>0.022941</td>\n",
       "      <td>0.057998</td>\n",
       "      <td>0.092985</td>\n",
       "      <td>0.226803</td>\n",
       "      <td>0.057732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043687</td>\n",
       "      <td>0.155308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.112068</td>\n",
       "      <td>0.199641</td>\n",
       "      <td>0.288044</td>\n",
       "      <td>0.058831</td>\n",
       "      <td>0.066767</td>\n",
       "      <td>0.140977</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>0.193045</td>\n",
       "      <td>0.218127</td>\n",
       "      <td>0.031614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049392</td>\n",
       "      <td>0.087422</td>\n",
       "      <td>0.030498</td>\n",
       "      <td>0.094621</td>\n",
       "      <td>0.145220</td>\n",
       "      <td>0.249341</td>\n",
       "      <td>0.073581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076266</td>\n",
       "      <td>0.184786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.148718</td>\n",
       "      <td>0.212346</td>\n",
       "      <td>0.311356</td>\n",
       "      <td>0.063071</td>\n",
       "      <td>0.078401</td>\n",
       "      <td>0.196763</td>\n",
       "      <td>0.026376</td>\n",
       "      <td>0.214301</td>\n",
       "      <td>0.236645</td>\n",
       "      <td>0.043057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059591</td>\n",
       "      <td>0.108358</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>0.133639</td>\n",
       "      <td>0.163191</td>\n",
       "      <td>0.275839</td>\n",
       "      <td>0.098730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188765</td>\n",
       "      <td>0.212577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.211416</td>\n",
       "      <td>0.247781</td>\n",
       "      <td>0.352280</td>\n",
       "      <td>0.075301</td>\n",
       "      <td>0.114802</td>\n",
       "      <td>0.307636</td>\n",
       "      <td>0.071061</td>\n",
       "      <td>0.256145</td>\n",
       "      <td>0.276989</td>\n",
       "      <td>0.073959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089746</td>\n",
       "      <td>0.145820</td>\n",
       "      <td>0.079803</td>\n",
       "      <td>0.206378</td>\n",
       "      <td>0.206515</td>\n",
       "      <td>0.360694</td>\n",
       "      <td>0.203084</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.273941</td>\n",
       "      <td>0.278711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  8100.000000  8100.000000  8100.000000  8100.000000  8100.000000   \n",
       "mean      0.116975     0.192254     0.270316     0.055375     0.067886   \n",
       "std       0.037431     0.030156     0.049625     0.010156     0.015720   \n",
       "min       0.024034     0.074731     0.129599     0.026587     0.022601   \n",
       "25%       0.087276     0.181227     0.227657     0.046315     0.056604   \n",
       "50%       0.112068     0.199641     0.288044     0.058831     0.066767   \n",
       "75%       0.148718     0.212346     0.311356     0.063071     0.078401   \n",
       "max       0.211416     0.247781     0.352280     0.075301     0.114802   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  8100.000000  8100.000000  8100.000000  8100.000000  8100.000000   \n",
       "mean      0.159494     0.025392     0.193006     0.220286     0.034857   \n",
       "std       0.048220     0.012440     0.026172     0.019965     0.012102   \n",
       "min       0.082664     0.009166     0.107948     0.156730     0.013670   \n",
       "25%       0.124366     0.017765     0.175341     0.205185     0.025137   \n",
       "50%       0.140977     0.020996     0.193045     0.218127     0.031614   \n",
       "75%       0.196763     0.026376     0.214301     0.236645     0.043057   \n",
       "max       0.307636     0.071061     0.256145     0.276989     0.073959   \n",
       "\n",
       "          ...               118          119          120          121  \\\n",
       "count     ...       8100.000000  8100.000000  8100.000000  8100.000000   \n",
       "mean      ...          0.046615     0.083562     0.035388     0.099931   \n",
       "std       ...          0.017693     0.031684     0.015684     0.043736   \n",
       "min       ...          0.003693     0.007219     0.004099     0.030014   \n",
       "25%       ...          0.036601     0.065289     0.022941     0.057998   \n",
       "50%       ...          0.049392     0.087422     0.030498     0.094621   \n",
       "75%       ...          0.059591     0.108358     0.047193     0.133639   \n",
       "max       ...          0.089746     0.145820     0.079803     0.206378   \n",
       "\n",
       "               122          123          124          125          126  \\\n",
       "count  8100.000000  8100.000000  8100.000000  8100.000000  8100.000000   \n",
       "mean      0.126097     0.253255     0.084274     0.000118     0.110998   \n",
       "std       0.049993     0.037211     0.040625     0.000357     0.076867   \n",
       "min       0.005417     0.169529     0.015992     0.000000     0.010813   \n",
       "25%       0.092985     0.226803     0.057732     0.000000     0.043687   \n",
       "50%       0.145220     0.249341     0.073581     0.000000     0.076266   \n",
       "75%       0.163191     0.275839     0.098730     0.000000     0.188765   \n",
       "max       0.206515     0.360694     0.203084     0.003603     0.273941   \n",
       "\n",
       "               127  \n",
       "count  8100.000000  \n",
       "mean      0.175048  \n",
       "std       0.055959  \n",
       "min       0.005754  \n",
       "25%       0.155308  \n",
       "50%       0.184786  \n",
       "75%       0.212577  \n",
       "max       0.278711  \n",
       "\n",
       "[8 rows x 128 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the simple statistics around the encodings\n",
    "dfstat = df.describe()\n",
    "dfstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146 146 146 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# See how they all cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "number_of_clusters=180\n",
    "km = KMeans(n_clusters=number_of_clusters)\n",
    "# Normally people fit the matrix\n",
    "km.fit(df)\n",
    "print(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ignore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ignore\n",
       "category        \n",
       "0             45\n",
       "1             45\n",
       "2             45\n",
       "3             45\n",
       "4             45\n",
       "5             45\n",
       "6             45\n",
       "7             45\n",
       "8             58\n",
       "9             45\n",
       "10            45\n",
       "11            45\n",
       "12            19\n",
       "13            45\n",
       "14            90\n",
       "15            44\n",
       "16            45\n",
       "17            45\n",
       "18            45\n",
       "19            45\n",
       "20            45\n",
       "21            45\n",
       "22            90\n",
       "23            45\n",
       "24            45\n",
       "25            45\n",
       "26            45\n",
       "27            17\n",
       "28            45\n",
       "29            45\n",
       "...          ...\n",
       "151           45\n",
       "152           45\n",
       "153           45\n",
       "154           45\n",
       "155           45\n",
       "156           25\n",
       "157           45\n",
       "158           45\n",
       "159           45\n",
       "160           45\n",
       "161           45\n",
       "162           45\n",
       "163           45\n",
       "164           45\n",
       "165           45\n",
       "166           45\n",
       "167           45\n",
       "168           23\n",
       "169           45\n",
       "170           35\n",
       "171           45\n",
       "172           45\n",
       "173           45\n",
       "174           26\n",
       "175           45\n",
       "176           45\n",
       "177           28\n",
       "178           45\n",
       "179           45\n",
       "180           45\n",
       "\n",
       "[181 rows x 1 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results to a dataframe - note that to get the index / filename, I had to include the first column\n",
    "results = pd.DataFrame({\n",
    "    'ignore': df[0],\n",
    "    'category': km.labels_\n",
    "})\n",
    "results.groupby('category').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the distance between images\n",
    "#  should be small distances between rotated images - ideally zero\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "distdf = pd.DataFrame(squareform(pdist(df.iloc[:, 1:])), columns = df.index.unique(), index=df.index.unique()) #, columns=df['image'].unique(), index=df['image'].unique())\n",
    "#distdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the covariance between the columns - not useful since it's unscaled. :-(\n",
    "#covdf = df.cov()*10000 #, columns = df.index.unique(), index=df.index.unique()) #, columns=df['image'].unique(), index=df['image'].unique())\n",
    "#covdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the correlation between the columns (which represent a global pool from the encoder convolutions)\n",
    "#   Correlation appears high, likely could do some PCA, but seems like tuning the autoencoder should be able to do that\n",
    "corrdf = df.corr() #, columns = df.index.unique(), index=df.index.unique()) #, columns=df['image'].unique(), index=df['image'].unique())\n",
    "#corrdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annoy for nearest neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Annoy database file\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "embedding_vector_size = 128\n",
    "\n",
    "f = embedding_vector_size      # Length of item vector that will be indexed\n",
    "t = AnnoyIndex(f)  \n",
    "\n",
    "for i in range(len(image_encoding_dict)):\n",
    "    t.add_item(i,image_encoding_dict[image_filename_dict[i]])\n",
    "    \n",
    "t.build(25) # 25 trees - need to explore what is a good setting here\n",
    "t.save(model_name + '.ann') # Save the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - images/100_0.jpg : 0.0\n",
      "36 - images/100_357.jpg : 0.010695723816752434\n",
      "15 - images/100_2.jpg : 0.013065899722278118\n",
      "44 - images/100_88.jpg : 0.02369992621243\n",
      "35 - images/100_353.jpg : 0.023706741631031036\n",
      "38 - images/100_44.jpg : 0.023786064237356186\n",
      "19 - images/100_223.jpg : 0.02655843272805214\n",
      "6 - images/100_132.jpg : 0.027235092595219612\n",
      "30 - images/100_317.jpg : 0.027703627943992615\n",
      "29 - images/100_313.jpg : 0.028032338246703148\n",
      "20 - images/100_227.jpg : 0.028359899297356606\n",
      "10 - images/100_159.jpg : 0.029006559401750565\n",
      "18 - images/100_218.jpg : 0.02905852161347866\n",
      "27 - images/100_290.jpg : 0.029115863144397736\n",
      "9 - images/100_155.jpg : 0.029223686084151268\n",
      "28 - images/100_294.jpg : 0.02988913096487522\n",
      "34 - images/100_346.jpg : 0.030223699286580086\n",
      "33 - images/100_334.jpg : 0.030382007360458374\n",
      "7 - images/100_138.jpg : 0.03130684792995453\n",
      "24 - images/100_263.jpg : 0.03253783658146858\n",
      "32 - images/100_332.jpg : 0.03276343643665314\n",
      "26 - images/100_284.jpg : 0.03321653977036476\n",
      "21 - images/100_229.jpg : 0.033313047140836716\n",
      "41 - images/100_70.jpg : 0.033452026546001434\n",
      "37 - images/100_37.jpg : 0.03356441482901573\n",
      "14 - images/100_198.jpg : 0.03389965742826462\n",
      "11 - images/100_165.jpg : 0.03413088619709015\n",
      "3 - images/100_11.jpg : 0.03432721272110939\n",
      "22 - images/100_245.jpg : 0.03509899973869324\n",
      "42 - images/100_77.jpg : 0.03526300564408302\n",
      "40 - images/100_59.jpg : 0.03532189875841141\n",
      "4 - images/100_111.jpg : 0.03538869693875313\n",
      "43 - images/100_78.jpg : 0.03567920997738838\n",
      "16 - images/100_210.jpg : 0.03633032739162445\n",
      "23 - images/100_247.jpg : 0.03670058026909828\n",
      "39 - images/100_57.jpg : 0.03681056201457977\n",
      "2 - images/100_105.jpg : 0.03728610649704933\n",
      "25 - images/100_280.jpg : 0.037586990743875504\n",
      "12 - images/100_166.jpg : 0.037595607340335846\n",
      "17 - images/100_211.jpg : 0.03787374496459961\n",
      "13 - images/100_168.jpg : 0.038194429129362106\n",
      "5 - images/100_115.jpg : 0.038440775126218796\n",
      "1 - images/100_101.jpg : 0.03868633508682251\n",
      "8 - images/100_146.jpg : 0.03916601464152336\n",
      "31 - images/100_327.jpg : 0.040877677500247955\n",
      "6135 - images/60_187.jpg : 0.05036814510822296\n"
     ]
    }
   ],
   "source": [
    "# Open Annoy database file get example\n",
    "\n",
    "sample_item_index = 0\n",
    "nn_count = 46 # count of nearest neighbors to find\n",
    "\n",
    "u = AnnoyIndex(f)\n",
    "u.load(model_name + '.ann') # super fast, will just mmap the file\n",
    "\n",
    "nn = u.get_nns_by_item(sample_item_index, nn_count) # will find the 5 nearest neighbors\n",
    "\n",
    "for i in nn:\n",
    "    distance = u.get_distance(sample_item_index, i)\n",
    "    print(str(i) + ' - ' + image_filename_dict[i] + ' : ' + str(distance))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/38_173.jpg\n",
      "Image to match is images/37_0.jpg\n",
      "images/37_0.jpg\n",
      "images/37_1.jpg\n",
      "images/37_6.jpg\n",
      "images/37_313.jpg\n",
      "images/37_43.jpg\n",
      "images/37_311.jpg\n",
      "images/37_269.jpg\n",
      "images/37_7.jpg\n",
      "images/37_89.jpg\n",
      "images/37_86.jpg\n",
      "images/37_71.jpg\n",
      "images/37_21.jpg\n",
      "images/37_267.jpg\n",
      "images/37_182.jpg\n",
      "images/37_30.jpg\n",
      "images/37_309.jpg\n",
      "images/38_173.jpg\n",
      "images/37_252.jpg\n",
      "images/37_93.jpg\n",
      "images/37_19.jpg\n",
      "images/37_142.jpg\n",
      "images/37_246.jpg\n",
      "images/37_253.jpg\n",
      "images/38_220.jpg\n",
      "images/38_43.jpg\n",
      "images/37_320.jpg\n",
      "images/37_154.jpg\n",
      "images/39_221.jpg\n",
      "images/37_165.jpg\n",
      "images/37_152.jpg\n",
      "images/38_312.jpg\n",
      "images/38_164.jpg\n",
      "images/37_77.jpg\n",
      "images/37_150.jpg\n",
      "images/37_151.jpg\n",
      "images/38_309.jpg\n",
      "images/38_101.jpg\n",
      "images/37_260.jpg\n",
      "images/38_163.jpg\n",
      "images/37_60.jpg\n",
      "images/38_265.jpg\n",
      "images/38_38.jpg\n",
      "images/38_89.jpg\n",
      "images/37_174.jpg\n",
      "images/38_132.jpg\n",
      "images/37_117.jpg\n",
      "Image to match is images/38_0.jpg\n",
      "images/38_0.jpg\n",
      "images/38_2.jpg\n",
      "images/38_49.jpg\n",
      "images/38_43.jpg\n",
      "images/38_312.jpg\n",
      "images/38_354.jpg\n",
      "images/38_89.jpg\n",
      "images/38_265.jpg\n",
      "images/38_309.jpg\n",
      "images/38_335.jpg\n",
      "images/38_272.jpg\n",
      "images/38_132.jpg\n",
      "images/38_163.jpg\n",
      "images/38_149.jpg\n",
      "images/38_112.jpg\n",
      "images/38_38.jpg\n",
      "images/38_8.jpg\n",
      "images/38_337.jpg\n",
      "images/37_117.jpg\n",
      "images/38_162.jpg\n",
      "images/38_164.jpg\n",
      "images/38_173.jpg\n",
      "images/38_152.jpg\n",
      "images/38_202.jpg\n",
      "images/37_182.jpg\n",
      "images/38_220.jpg\n",
      "images/37_71.jpg\n",
      "images/38_341.jpg\n",
      "images/39_338.jpg\n",
      "images/38_330.jpg\n",
      "images/38_304.jpg\n",
      "images/38_137.jpg\n",
      "images/39_340.jpg\n",
      "images/38_101.jpg\n",
      "images/38_296.jpg\n",
      "images/37_119.jpg\n",
      "images/39_132.jpg\n",
      "images/38_145.jpg\n",
      "images/38_207.jpg\n",
      "images/38_286.jpg\n",
      "images/38_141.jpg\n",
      "images/37_165.jpg\n",
      "images/37_252.jpg\n",
      "images/38_287.jpg\n",
      "images/38_214.jpg\n",
      "Number of pills with any errors:  30 out of 180.0 pills\n",
      "Number of pill image errors:      101 out of 8100 images\n",
      "Number of pills with errors in 20:       2 out of 180.0 pills\n",
      "Number of pill image errors in top 20:   2 out of 8100 images\n"
     ]
    }
   ],
   "source": [
    "# Calculate some metric of goodness. Basically, see how many pills clump when rotated. \n",
    "#   In other words, the 4 closest pills to pill 219 should be itself plus the three rotations of it.\n",
    "\n",
    "number_rotations = 45 # the total images for each pill\n",
    "top_n = 20 # only get the top n images for comparison - not looking for 100% perfection, just in top n results\n",
    "\n",
    "same_images={key: [key+i for i in range(number_rotations)] for key in image_filename_dict if key % number_rotations == 0}\n",
    "same_images # key is first value, value is list with all the similar images\n",
    "\n",
    "incorrect_pills = 0 #sum of count of pills\n",
    "incorrect_pill_images = 0 # sum of ALL the mistakes\n",
    "\n",
    "incorrect_pills_top_n = 0\n",
    "incorrect_pill_images_top_n = 0\n",
    "\n",
    "total_pills = len(image_filename_dict)/number_rotations\n",
    "total_pill_images = len(image_filename_dict)\n",
    "\n",
    "for key, value in same_images.items():\n",
    "    top_matches = u.get_nns_by_item(key, number_rotations)\n",
    "    mismatches = set(value)-set(top_matches)\n",
    "    if len(mismatches) != 0:\n",
    "        incorrect_pills += 1\n",
    "        incorrect_pill_images += len(mismatches)\n",
    "    \n",
    "    top_n_mismatches = set(top_matches[0:top_n])-set(value)\n",
    "    if len(top_n_mismatches) != 0:\n",
    "        incorrect_pills_top_n += 1\n",
    "        incorrect_pill_images_top_n += len(top_n_mismatches)  \n",
    "        \n",
    "        print(image_filename_dict[top_n_mismatches.pop()])\n",
    "        print(f'Image to match is {image_filename_dict[value[0]]}')\n",
    "        for i in top_matches:\n",
    "            print(image_filename_dict[i])\n",
    "        \n",
    "\n",
    "print(f'Number of pills with any errors:  {incorrect_pills} out of {total_pills} pills')\n",
    "print(f\"Number of pill image errors:      {incorrect_pill_images} out of {total_pill_images} images\")\n",
    "\n",
    "print(f'Number of pills with errors in {top_n}:       {incorrect_pills_top_n} out of {total_pills} pills')\n",
    "print(f\"Number of pill image errors in top {top_n}:   {incorrect_pill_images_top_n} out of {total_pill_images} images\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4}\n",
      "{1, 4, 5, 6, 7}\n",
      "{45}\n",
      "{45}\n",
      "[3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "same = [0,1,2,3,4,5,6,7]\n",
    "top8 = [3,2,1,0,5,7,6,45]\n",
    "top4 = [0,3,2,45]\n",
    "z = set(same)-set(top8)\n",
    "print(z)\n",
    "z2 = set(same)-set(top4)\n",
    "print(z2)\n",
    "z3 = set(top8)-set(same)\n",
    "print(z3)\n",
    "z4 = set(top4)-set(same)\n",
    "print(z4)\n",
    "print(top8[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
