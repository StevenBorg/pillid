{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Autoencoder with Annoy for CBIR\n",
    "\n",
    "## Grayscale 32x64 pixels, 4x8x128 encoder convolutions, GlobalAverage to 128 dimensions\n",
    "\n",
    "#### Results: Grayscale, even at this resolution seems MUCH better than Black and White (pure through resize or not) or Edge Detection. Unexpected.  But MUCH better at grouping pills through rotations to one another. \n",
    "\n",
    "\n",
    "From Annoy:\\ - see below:\n",
    "Number of incorrect pills:       0 out of 24 pills\n",
    "Number of incorrect pill images: 0 out of 96 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15081321793875527189\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#target_image_size = (64,112)\n",
    "target_image_size = (32,64)\n",
    "\n",
    "color_mode='grayscale' # 'grayscale' or 'rgb'\n",
    "\n",
    "if color_mode=='grayscale':\n",
    "    target_image_size_3D = (target_image_size[0], target_image_size[1], 1)\n",
    "else: #then rgb\n",
    "    target_image_size_3D = (target_image_size[0], target_image_size[1], 3)\n",
    "\n",
    "batch_size_training = 32\n",
    "batch_size_validation = 32\n",
    "\n",
    "# Directories\n",
    "image_dir_base = 'data_with_rotations'\n",
    "image_dir_training = image_dir_base + '/train'\n",
    "image_dir_validation = image_dir_base + '/validate'\n",
    "image_dir_testing = image_dir_base + '/test'\n",
    "image_dir_samples = image_dir_testing\n",
    "\n",
    "# Training variables\n",
    "training_steps_per_epoch = 200\n",
    "training_number_of_epoch = 100\n",
    "validation_steps = 200\n",
    "training_early_stop_patience = 5\n",
    "\n",
    "# Model name to save\n",
    "model_name='autoencoder_v1.0_36x64-4x8x128'\n",
    "\n",
    "# Model name to load for Testing\n",
    "model_name_pretrained = model_name\n",
    "\n",
    "# Encoder model name\n",
    "model_name_encoder = model_name + '-encoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required items for training\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Dense, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras import callbacks\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, matplotlib, cv2, etc only print in greyscale \n",
    "#   when you have 3 color axes (RGB) all set to make the image look grey.\n",
    "#   But Keras loads greyscale images with only a single number (to optimize training, etc)\n",
    "#   So, we need to convert any Keras greyscale images to have 3 values\n",
    "\n",
    "def display_image(single_image):\n",
    "    #check to see if image is rgb\n",
    "    if (np.shape(single_image)[-1]==3):\n",
    "        plt.imshow(single_image)\n",
    "    if (np.shape(single_image)[-1]==1):\n",
    "        si = np.concatenate((single_image,single_image,single_image), axis=2)\n",
    "        plt.imshow(si)\n",
    "        \n",
    "def get_image(single_image):\n",
    "    #check to see if image is rgb\n",
    "    if (np.shape(single_image)[-1]==3):\n",
    "        return single_image\n",
    "    if (np.shape(single_image)[-1]==1):\n",
    "        si = np.concatenate((single_image,single_image,single_image), axis=2)\n",
    "        return si\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageDataGenerators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 722 images belonging to 1 classes.\n",
      "Found 191 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Created the Train and Validation image generators\n",
    "\n",
    "# Load the data (in that case MNIST)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        #shear_range=0.05,\n",
    "        #zoom_range=0.01,\n",
    "        #rotation_range=5.00,\n",
    "        #height_shift_range=0.10,\n",
    "        #width_shift_range=0.10,\n",
    "        rescale=1. / 255,\n",
    "        horizontal_flip=False)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        image_dir_training,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=batch_size_training,\n",
    "        class_mode='input',\n",
    "        color_mode=color_mode)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        image_dir_validation,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=batch_size_validation,\n",
    "        class_mode='input',\n",
    "        color_mode=color_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an Sample image generator and get sample images to use throughout all the training for visualization\n",
    "\n",
    "\n",
    "# Create callback function to use later.\n",
    "sample_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "sample_generator = sample_datagen.flow_from_directory(\n",
    "        image_dir_samples,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=16,\n",
    "        class_mode='input',\n",
    "        color_mode=color_mode)\n",
    "\n",
    "next_batch = next(sample_generator)\n",
    "sample_images = next_batch[0]\n",
    "test_image=sample_images[1]\n",
    "ti = test_image\n",
    "\n",
    "#\n",
    "display_image(test_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model for Training - Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    \n",
    "    \n",
    "    #Input\n",
    "    input_img = Input(shape=target_image_size_3D, name='input')  # adapt this if using `channels_first` image data format\n",
    "    \n",
    "     # Layer 10\n",
    "    x = Conv2D(16, (5, 5), activation='relu', padding='same', strides=(2,2))(input_img)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Layer 20\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', strides=(2,2))(x)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "#    # Added\n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "#    \n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # Layer 30\n",
    "\n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Layer 40\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same', name='encoder')(x)\n",
    "\n",
    "    # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "    # Uplayer 40\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    # Uplayer 30\n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "\n",
    "#    #Added\n",
    "#    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = UpSampling2D((2, 2))(x)  \n",
    "#    \n",
    "#    x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "#    #x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "#    x = UpSampling2D((2, 2))(x)  \n",
    "    \n",
    "    # Uplayer 20\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    # Uplayer 10\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    # Output\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='decoded')(x)\n",
    "\n",
    "\n",
    "    autoencoder = Model(input_img, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "    print(autoencoder.outputs)\n",
    "        \n",
    "    return autoencoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback to write sample images to disk\n",
    "class ProgressCallback(callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, sample_image):\n",
    "        self.sample_image = sample_image\n",
    "        self.image4d = self.sample_image[None,:] # predict needs a batch of images (shape=4). This adds a dimension  \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        processed_images = self.model.predict(x=[self.image4d],batch_size=1)\n",
    "         \n",
    "        # plot the image and save it\n",
    "        f = plt.figure()\n",
    "        f.add_subplot(1, 2, 1)  # this line outputs images side-by-side\n",
    "        sim = get_image(self.sample_image)\n",
    "        plt.imshow(sim)\n",
    "        f.add_subplot(1, 2, 2)  # this line outputs images side-by-side\n",
    "        pim = get_image(processed_images[0])\n",
    "        plt.imshow(pim)\n",
    "        plt.suptitle('Epoch ' + str(epoch))\n",
    "        filename = 'epoch-' + str(epoch) + '.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model_to_train):\n",
    "    progress = ProgressCallback(sample_image=sample_images[0])\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=training_early_stop_patience,\n",
    "                              verbose=0, mode='auto')\n",
    "    model_to_train.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=training_steps_per_epoch,\n",
    "        epochs=training_number_of_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False),progress,early_stop])\n",
    "    \n",
    "    model_to_train.save(model_name + '.h5')\n",
    "    \n",
    "    return model_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'decoded_1/Sigmoid:0' shape=(?, 32, 64, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "x = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 32, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 32, 16)        416       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 16, 32)         4640      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 16, 128)        36992     \n",
      "_________________________________________________________________\n",
      "encoder (MaxPooling2D)       (None, 4, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 16, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 16, 32)         36896     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 32, 16)        4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 32, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "decoded (Conv2D)             (None, 32, 64, 1)         145       \n",
      "=================================================================\n",
      "Total params: 231,297\n",
      "Trainable params: 231,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'encoder_1/MaxPool:0' shape=(?, 4, 8, 128) dtype=float32>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show encoder output size (before global pooling)\n",
    "e = x.get_layer('encoder')\n",
    "e.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 40/200 [=====>........................] - ETA: 19s - loss: 0.4904"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-3f97ab21a130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-d0de1e3f3a8d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_to_train)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False),progress,early_stop])\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_to_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder = x\n",
    "autoencoder = train_model(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install jupyter-tensorboard\n",
    "for i in range(3):\n",
    "    pass\n",
    "    #autoencoder = train_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    #autoencoder = train_model(x)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "#import cv2\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model :\n",
      "Model loaded in:  1.125614881515503\n"
     ]
    }
   ],
   "source": [
    "# Load the model trained above\n",
    "print('Loading model :')\n",
    "t0 = time.time()\n",
    "autoencoder = load_model(model_name_pretrained + '.h5')\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoder').output)\n",
    "t1 = time.time()\n",
    "print('Model loaded in: ', t1-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convenience methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_directory_images_generator:\n",
    "    def __init__(self, sourcedir='data_with_rotations/test', batch_size=16, color_mode='grayscale', target_image_size=(100,100)):\n",
    "        self.batch_size = batch_size\n",
    "        self.sourcedir = sourcedir\n",
    "        self.color_mode = color_mode\n",
    "        self.target_image_size = target_image_size\n",
    "        \n",
    "        self.test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        self.test_generator = self.test_datagen.flow_from_directory(\n",
    "                sourcedir,\n",
    "                target_size=target_image_size,\n",
    "                batch_size=self.batch_size,\n",
    "                class_mode='input',\n",
    "                color_mode=self.color_mode,\n",
    "                shuffle=False)\n",
    "        \n",
    "        self.n = self.test_generator.n\n",
    "        self.filenames = self.test_generator.filenames   \n",
    "        self.current_batch = 0\n",
    "        self.max_batch = int(self.n / self.batch_size)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        bi = self.test_generator.batch_index\n",
    "        bs = self.test_generator.batch_size\n",
    "        batch_file_names = self.test_generator.filenames[bi*bs:bi*bs+bs]\n",
    "        return (next(self.test_generator), batch_file_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model from the autoencoder, only up to the embedding layer\n",
    "enc_model = Model(autoencoder.input, autoencoder.get_layer('encoder').output)\n",
    "\n",
    "x1 = enc_model.get_layer('encoder').output\n",
    "#x1 = GlobalMaxPooling2D(name='flat')(x1)\n",
    "x1 = GlobalAveragePooling2D(name='flat')(x1)\n",
    "encoder = Model(enc_model.input, x1)\n",
    "\n",
    "# save the model to disk for reuse later\n",
    "encoder.save(model_name_encoder + '.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1080 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary with all filenames (keys) and predicted encodings (values)\n",
    "\n",
    "image_dir_testing = 'data_with_many_rotations/test'\n",
    "image_encoding_dict = {}\n",
    "\n",
    "test_images = all_directory_images_generator(batch_size=45, target_image_size=target_image_size, sourcedir=image_dir_testing)\n",
    "bs = test_images.batch_size\n",
    "\n",
    "for i in range(test_images.max_batch):\n",
    "    images_both_x_and_y, names = next(test_images)\n",
    "    images = images_both_x_and_y[0]\n",
    "    encodings = encoder.predict(images,batch_size=bs)\n",
    "    for j in range(bs):\n",
    "        image_encoding_dict[names[j]]=encodings[j]\n",
    "        \n",
    "\n",
    "\n",
    "# Create dictionary with index integers (keys) and filenames (values)\n",
    "#   This is needed later for Annoy, since it uses integers as item keys, and we have to map back to a filename\n",
    "image_filename_dict = {}\n",
    "\n",
    "i = 0\n",
    "for key, value in image_encoding_dict.items():\n",
    "    image_filename_dict[i]=key\n",
    "    i = i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/229.jpg\n"
     ]
    }
   ],
   "source": [
    "# Let's just view a few input images and output images from the autoencoder\n",
    "#   This includes the decoder part of the autoencoder, so we'll still see an image\n",
    "\n",
    "next_batch, next_filenames = next(test_images) # Use the test_images defined above\n",
    "#next_batch = next(test_generator)\n",
    "images = next_batch[0]\n",
    "first_image=images[0]\n",
    "second_image=images[1]\n",
    "\n",
    "plt.imshow(get_image(first_image))\n",
    "plt.show()\n",
    "\n",
    "#plt.imshow(second_image)\n",
    "#plt.show()\n",
    "\n",
    "recreated_pill = autoencoder.predict(x=images,batch_size=32)\n",
    "\n",
    "plt.imshow(get_image(recreated_pill[0]))\n",
    "plt.show()\n",
    "print(next_filenames[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feel for the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataframe from the dictionary\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(image_encoding_dict, orient='index')\n",
    "\n",
    "#df['filename'] = df.index\n",
    "#df['image']= df['filename'].apply(lambda x: x.split('/')[-1])\n",
    "#HOLY COW!  All of these are so close together. :-(  Can't differentiate one from another easily - Deeper network help?\n",
    "#df.to_csv('96_images_32x64_AvgPool_4x8.csv')\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "      <td>1080.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.089129</td>\n",
       "      <td>0.199720</td>\n",
       "      <td>0.299069</td>\n",
       "      <td>0.067711</td>\n",
       "      <td>0.084535</td>\n",
       "      <td>0.228169</td>\n",
       "      <td>0.037134</td>\n",
       "      <td>0.213250</td>\n",
       "      <td>0.239196</td>\n",
       "      <td>0.038363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048760</td>\n",
       "      <td>0.067831</td>\n",
       "      <td>0.055833</td>\n",
       "      <td>0.165023</td>\n",
       "      <td>0.111613</td>\n",
       "      <td>0.309485</td>\n",
       "      <td>0.157215</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.176088</td>\n",
       "      <td>0.209207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.015204</td>\n",
       "      <td>0.023254</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>0.057137</td>\n",
       "      <td>0.010572</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.014214</td>\n",
       "      <td>0.012715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012479</td>\n",
       "      <td>0.022952</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.027243</td>\n",
       "      <td>0.040096</td>\n",
       "      <td>0.035205</td>\n",
       "      <td>0.040117</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.053846</td>\n",
       "      <td>0.030329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.034368</td>\n",
       "      <td>0.158413</td>\n",
       "      <td>0.221333</td>\n",
       "      <td>0.054070</td>\n",
       "      <td>0.034831</td>\n",
       "      <td>0.087620</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>0.152543</td>\n",
       "      <td>0.199884</td>\n",
       "      <td>0.017431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.026136</td>\n",
       "      <td>0.021812</td>\n",
       "      <td>0.097821</td>\n",
       "      <td>0.049362</td>\n",
       "      <td>0.211594</td>\n",
       "      <td>0.040498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051780</td>\n",
       "      <td>0.130105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.068474</td>\n",
       "      <td>0.191526</td>\n",
       "      <td>0.285592</td>\n",
       "      <td>0.064492</td>\n",
       "      <td>0.069741</td>\n",
       "      <td>0.196910</td>\n",
       "      <td>0.029152</td>\n",
       "      <td>0.201712</td>\n",
       "      <td>0.229989</td>\n",
       "      <td>0.028948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039071</td>\n",
       "      <td>0.050384</td>\n",
       "      <td>0.049888</td>\n",
       "      <td>0.149248</td>\n",
       "      <td>0.071114</td>\n",
       "      <td>0.295683</td>\n",
       "      <td>0.146044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142714</td>\n",
       "      <td>0.194897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.087693</td>\n",
       "      <td>0.198380</td>\n",
       "      <td>0.301804</td>\n",
       "      <td>0.068416</td>\n",
       "      <td>0.082189</td>\n",
       "      <td>0.235193</td>\n",
       "      <td>0.037379</td>\n",
       "      <td>0.213766</td>\n",
       "      <td>0.240371</td>\n",
       "      <td>0.034184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047987</td>\n",
       "      <td>0.074602</td>\n",
       "      <td>0.057068</td>\n",
       "      <td>0.166854</td>\n",
       "      <td>0.115520</td>\n",
       "      <td>0.314147</td>\n",
       "      <td>0.172054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182810</td>\n",
       "      <td>0.211889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.105503</td>\n",
       "      <td>0.209246</td>\n",
       "      <td>0.314875</td>\n",
       "      <td>0.070705</td>\n",
       "      <td>0.102867</td>\n",
       "      <td>0.275357</td>\n",
       "      <td>0.046292</td>\n",
       "      <td>0.227672</td>\n",
       "      <td>0.249879</td>\n",
       "      <td>0.049564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057489</td>\n",
       "      <td>0.082836</td>\n",
       "      <td>0.063204</td>\n",
       "      <td>0.189028</td>\n",
       "      <td>0.145335</td>\n",
       "      <td>0.337643</td>\n",
       "      <td>0.185273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221477</td>\n",
       "      <td>0.219053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.144875</td>\n",
       "      <td>0.235480</td>\n",
       "      <td>0.343521</td>\n",
       "      <td>0.078364</td>\n",
       "      <td>0.129355</td>\n",
       "      <td>0.323091</td>\n",
       "      <td>0.058162</td>\n",
       "      <td>0.248079</td>\n",
       "      <td>0.276714</td>\n",
       "      <td>0.068920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086853</td>\n",
       "      <td>0.117727</td>\n",
       "      <td>0.081802</td>\n",
       "      <td>0.206202</td>\n",
       "      <td>0.186074</td>\n",
       "      <td>0.364691</td>\n",
       "      <td>0.200402</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.271696</td>\n",
       "      <td>0.281653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  1080.000000  1080.000000  1080.000000  1080.000000  1080.000000   \n",
       "mean      0.089129     0.199720     0.299069     0.067711     0.084535   \n",
       "std       0.027748     0.015204     0.023254     0.004198     0.020491   \n",
       "min       0.034368     0.158413     0.221333     0.054070     0.034831   \n",
       "25%       0.068474     0.191526     0.285592     0.064492     0.069741   \n",
       "50%       0.087693     0.198380     0.301804     0.068416     0.082189   \n",
       "75%       0.105503     0.209246     0.314875     0.070705     0.102867   \n",
       "max       0.144875     0.235480     0.343521     0.078364     0.129355   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  1080.000000  1080.000000  1080.000000  1080.000000  1080.000000   \n",
       "mean      0.228169     0.037134     0.213250     0.239196     0.038363   \n",
       "std       0.057137     0.010572     0.018018     0.014214     0.012715   \n",
       "min       0.087620     0.012865     0.152543     0.199884     0.017431   \n",
       "25%       0.196910     0.029152     0.201712     0.229989     0.028948   \n",
       "50%       0.235193     0.037379     0.213766     0.240371     0.034184   \n",
       "75%       0.275357     0.046292     0.227672     0.249879     0.049564   \n",
       "max       0.323091     0.058162     0.248079     0.276714     0.068920   \n",
       "\n",
       "          ...               118          119          120          121  \\\n",
       "count     ...       1080.000000  1080.000000  1080.000000  1080.000000   \n",
       "mean      ...          0.048760     0.067831     0.055833     0.165023   \n",
       "std       ...          0.012479     0.022952     0.011564     0.027243   \n",
       "min       ...          0.019481     0.026136     0.021812     0.097821   \n",
       "25%       ...          0.039071     0.050384     0.049888     0.149248   \n",
       "50%       ...          0.047987     0.074602     0.057068     0.166854   \n",
       "75%       ...          0.057489     0.082836     0.063204     0.189028   \n",
       "max       ...          0.086853     0.117727     0.081802     0.206202   \n",
       "\n",
       "               122          123          124          125          126  \\\n",
       "count  1080.000000  1080.000000  1080.000000  1080.000000  1080.000000   \n",
       "mean      0.111613     0.309485     0.157215     0.000109     0.176088   \n",
       "std       0.040096     0.035205     0.040117     0.000355     0.053846   \n",
       "min       0.049362     0.211594     0.040498     0.000000     0.051780   \n",
       "25%       0.071114     0.295683     0.146044     0.000000     0.142714   \n",
       "50%       0.115520     0.314147     0.172054     0.000000     0.182810   \n",
       "75%       0.145335     0.337643     0.185273     0.000000     0.221477   \n",
       "max       0.186074     0.364691     0.200402     0.003595     0.271696   \n",
       "\n",
       "               127  \n",
       "count  1080.000000  \n",
       "mean      0.209207  \n",
       "std       0.030329  \n",
       "min       0.130105  \n",
       "25%       0.194897  \n",
       "50%       0.211889  \n",
       "75%       0.219053  \n",
       "max       0.281653  \n",
       "\n",
       "[8 rows x 128 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the simple statistics around the encodings\n",
    "dfstat = df.describe()\n",
    "dfstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 21 21 ... 12 12 12]\n"
     ]
    }
   ],
   "source": [
    "# See how they all cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "number_of_clusters=24\n",
    "km = KMeans(n_clusters=number_of_clusters)\n",
    "# Normally people fit the matrix\n",
    "km.fit(df)\n",
    "print(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ignore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ignore\n",
       "category        \n",
       "0             45\n",
       "1             45\n",
       "2             45\n",
       "3             45\n",
       "4             45\n",
       "5             45\n",
       "6             45\n",
       "7             45\n",
       "8             45\n",
       "9             45\n",
       "10            45\n",
       "11            45\n",
       "12            45\n",
       "13            45\n",
       "14            45\n",
       "15            45\n",
       "16            45\n",
       "17            45\n",
       "18            45\n",
       "19            45\n",
       "20            45\n",
       "21            45\n",
       "22            45\n",
       "23            45"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results to a dataframe - note that to get the index / filename, I had to include the first column\n",
    "results = pd.DataFrame({\n",
    "    'ignore': df[0],\n",
    "    'category': km.labels_\n",
    "})\n",
    "results.groupby('category').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the distance between images\n",
    "#  should be small distances between rotated images - ideally zero\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "distdf = pd.DataFrame(squareform(pdist(df.iloc[:, 1:])), columns = df.index.unique(), index=df.index.unique()) #, columns=df['image'].unique(), index=df['image'].unique())\n",
    "#distdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the covariance between the columns - not useful since it's unscaled. :-(\n",
    "#covdf = df.cov()*10000 #, columns = df.index.unique(), index=df.index.unique()) #, columns=df['image'].unique(), index=df['image'].unique())\n",
    "#covdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the correlation between the columns (which represent a global pool from the encoder convolutions)\n",
    "#   Correlation appears high, likely could do some PCA, but seems like tuning the autoencoder should be able to do that\n",
    "corrdf = df.corr() #, columns = df.index.unique(), index=df.index.unique()) #, columns=df['image'].unique(), index=df['image'].unique())\n",
    "#corrdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annoy for nearest neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Annoy database file\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "embedding_vector_size = 128\n",
    "\n",
    "f = embedding_vector_size      # Length of item vector that will be indexed\n",
    "t = AnnoyIndex(f)  \n",
    "\n",
    "for i in range(len(image_encoding_dict)):\n",
    "    t.add_item(i,image_encoding_dict[image_filename_dict[i]])\n",
    "    \n",
    "t.build(25) # 25 trees - need to explore what is a good setting here\n",
    "t.save(model_name + '.ann') # Save the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - images/229.jpg : 0.0\n",
      "11 - images/229_2.jpg : 0.012086973525583744\n",
      "26 - images/229_3.jpg : 0.01648597978055477\n",
      "38 - images/229_6.jpg : 0.027209298685193062\n",
      "29 - images/229_311.jpg : 0.030696075409650803\n",
      "31 - images/229_338.jpg : 0.030803890898823738\n"
     ]
    }
   ],
   "source": [
    "# Open Annoy database file get example\n",
    "\n",
    "sample_item_index = 0\n",
    "nn_count = 6 # count of nearest neighbors to find\n",
    "\n",
    "u = AnnoyIndex(f)\n",
    "u.load(model_name + '.ann') # super fast, will just mmap the file\n",
    "\n",
    "nn = u.get_nns_by_item(sample_item_index, nn_count) # will find the 5 nearest neighbors\n",
    "\n",
    "for i in nn:\n",
    "    distance = u.get_distance(sample_item_index, i)\n",
    "    print(str(i) + ' - ' + image_filename_dict[i] + ' : ' + str(distance))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incorrect pills:       0 out of 24.0 pills\n",
      "Number of incorrect pill images: 0 out of 1080 images\n"
     ]
    }
   ],
   "source": [
    "# Calculate some metric of goodness. Basically, see how many pills clump when rotated. \n",
    "#   In other words, the 4 closest pills to pill 219 should be itself plus the three rotations of it.\n",
    "\n",
    "number_rotations = 45 # the total images for each pill\n",
    "same_images={key: [key+i for i in range(number_rotations)] for key in image_filename_dict if key % number_rotations == 0}\n",
    "same_images # key is first value, value is list with all the similar images\n",
    "\n",
    "incorrect_pills = 0 #sum of count of pills\n",
    "incorrect_pill_images = 0 # sum of ALL the mistakes\n",
    "\n",
    "total_pills = len(image_filename_dict)/number_rotations\n",
    "total_pill_images = len(image_filename_dict)\n",
    "\n",
    "for key, value in same_images.items():\n",
    "    top_matches = u.get_nns_by_item(key, number_rotations)\n",
    "    mismatches = set(value)-set(top_matches)\n",
    "    if len(mismatches) != 0:\n",
    "        incorrect_pills += 1\n",
    "        incorrect_pill_images += len(mismatches)\n",
    "\n",
    "print(f'Number of incorrect pills:       {incorrect_pills} out of {total_pills} pills')\n",
    "print(f\"Number of incorrect pill images: {incorrect_pill_images} out of {total_pill_images} images\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
